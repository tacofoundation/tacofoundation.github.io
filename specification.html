<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>The TACO specification | The TACO Specification</title>
    <meta name="description" content="Every EO problem deserves a TACO.">
    <meta name="generator" content="VitePress v1.6.3">
    <link rel="preload stylesheet" href="/assets/style.yNO1IXAn.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    
    <script type="module" src="/assets/app.BvwnuyIP.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/theme.DhcKzeZs.js">
    <link rel="modulepreload" href="/assets/chunks/framework.BjlC_BXf.js">
    <link rel="modulepreload" href="/assets/specification.md.gSG_sY5t.lean.js">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-5d98c3a5><!--[--><!--]--><!--[--><span tabindex="-1" data-v-0b0ada53></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-0b0ada53>Skip to content</a><!--]--><!----><header class="VPNav" data-v-5d98c3a5 data-v-ae24b3ad><div class="VPNavBar" data-v-ae24b3ad data-v-6aa21345><div class="wrapper" data-v-6aa21345><div class="container" data-v-6aa21345><div class="title" data-v-6aa21345><div class="VPNavBarTitle" data-v-6aa21345 data-v-1168a8e4><a class="title" href="/" data-v-1168a8e4><!--[--><!--]--><!----><span data-v-1168a8e4>The TACO Specification</span><!--[--><!--]--></a></div></div><div class="content" data-v-6aa21345><div class="content-body" data-v-6aa21345><!--[--><!--]--><div class="VPNavBarSearch search" data-v-6aa21345><!----></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-6aa21345 data-v-dc692963><span id="main-nav-aria-label" class="visually-hidden" data-v-dc692963> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>Home</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink active" href="/specification.html" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>Specification</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/documentation.html" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>Documentation</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/contribute.html" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>Contributing</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/catalogue.html" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>Catalog</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/publications.html" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>Publications</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-6aa21345 data-v-6c893767><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-6c893767 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-6aa21345 data-v-0394ad82 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/tacofoundation" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><a class="VPSocialLink no-icon" href="https://huggingface.co/tacofoundation" aria-label="hugging face" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-hugging face"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-6aa21345 data-v-bb2aa2f0 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-cf11d7a2><span class="vpi-more-horizontal icon" data-v-cf11d7a2></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><!----><!--[--><!--[--><!----><div class="group" data-v-bb2aa2f0><div class="item appearance" data-v-bb2aa2f0><p class="label" data-v-bb2aa2f0>Appearance</p><div class="appearance-action" data-v-bb2aa2f0><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-bb2aa2f0 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div></div></div><div class="group" data-v-bb2aa2f0><div class="item social-links" data-v-bb2aa2f0><div class="VPSocialLinks social-links-list" data-v-bb2aa2f0 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/tacofoundation" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><a class="VPSocialLink no-icon" href="https://huggingface.co/tacofoundation" aria-label="hugging face" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-hugging face"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-6aa21345 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><div class="divider" data-v-6aa21345><div class="divider-line" data-v-6aa21345></div></div></div><!----></header><div class="VPLocalNav empty fixed" data-v-5d98c3a5 data-v-a6f0e41e><div class="container" data-v-a6f0e41e><!----><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-a6f0e41e data-v-8a42e2b4><button data-v-8a42e2b4>Return to top</button><!----></div></div></div><!----><div class="VPContent" id="VPContent" data-v-5d98c3a5 data-v-1428d186><div class="VPDoc has-aside" data-v-1428d186 data-v-39a288b8><!--[--><!--]--><div class="container" data-v-39a288b8><div class="aside" data-v-39a288b8><div class="aside-curtain" data-v-39a288b8></div><div class="aside-container" data-v-39a288b8><div class="aside-content" data-v-39a288b8><div class="VPDocAside" data-v-39a288b8 data-v-3f215769><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-3f215769 data-v-a5bbad30><div class="content" data-v-a5bbad30><div class="outline-marker" data-v-a5bbad30></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-a5bbad30>On this page</div><ul class="VPDocOutlineItem root" data-v-a5bbad30 data-v-b933a997><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-3f215769></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-39a288b8><div class="content-container" data-v-39a288b8><!--[--><!--]--><main class="main" data-v-39a288b8><div style="position:relative;" class="vp-doc _specification" data-v-39a288b8><div><h1 id="the-taco-specification" tabindex="-1">The TACO specification <a class="header-anchor" href="#the-taco-specification" aria-label="Permalink to &quot;The TACO specification&quot;">​</a></h1><p>The terms &quot;MUST&quot;, &quot;MUST NOT&quot;, &quot;REQUIRED&quot;, &quot;SHALL&quot;, &quot;SHALL NOT&quot;, &quot;SHOULD&quot;, &quot;SHOULD NOT&quot;, &quot;RECOMMENDED&quot;, &quot;MAY&quot;, and &quot;OPTIONAL&quot; in this document follow the definitions from <a href="https://www.ietf.org/rfc/rfc2119.txt" target="_blank" rel="noreferrer">RFC 2119</a>. The term &quot;data user&quot; refers to any individual or system accessing the data, &quot;data provider&quot; refers to the entity responsible for offering the data.</p><h2 id="version-and-schema" tabindex="-1">Version and schema <a class="header-anchor" href="#version-and-schema" aria-label="Permalink to &quot;Version and schema&quot;">​</a></h2><p><strong>This is version 0.2.0 of the TACO specification</strong>. The core data structures are defined through <a href="https://github.com/tacofoundation/specification/tree/main/diagrams" target="_blank" rel="noreferrer">UML diagrams</a> for easy inspection. To validate TACO-compliant files, use the <a href="https://github.com/tacofoundation/taco-toolbox" target="_blank" rel="noreferrer">TACO Toolbox</a>, which is available in C++, Python, R, and Julia. <strong>Future versions must remain backward-compatible with this one.</strong></p><h3 id="summary" tabindex="-1">Summary <a class="header-anchor" href="#summary" aria-label="Permalink to &quot;Summary&quot;">​</a></h3><p>Geospatial communities have long relied on standardised formats for raster and vector data, enabling interoperability, stable tool development, and long-term data preservation. In contrast, Artificial Intelligence (AI)-ready datasets, particularly those derived from Earth Observation (EO), lack equivalent conventions. As a result, data producers often adopt ad hoc file structures, loosely defined formats, and inconsistent semantic encodings. This fragmentation hinders interoperability, complicates reuse, and undermines reproducibility. We argue that the lack of a standard format represents a structural bottleneck to scalable scientific progress, especially in the era of foundation models, where diverse datasets must be combined for effective training and performance evaluation in downstream tasks. To address this, we introduce TACO: a comprehensive specification that defines a formal data model, a cloud-optimized on-disk layout, and an API for creating and accessing AI-ready EO datasets.</p><h3 id="introduction" tabindex="-1">Introduction <a class="header-anchor" href="#introduction" aria-label="Permalink to &quot;Introduction&quot;">​</a></h3><p>The rapid increase in Earth Observation (EO) data, combined with advances in AI and cloud computing, has unlocked new opportunities for scientific discovery and operational monitoring (<em>Montillet et al., 2024</em>; *Eyring et al., 2024; <em>Hagos et al., 2022</em>). Modern applications range from methane superemitter detection (<em>Vaughan et al., 2024</em>) and burned area estimation (<em>Ribeiro et al., 2023</em>) to biodiversity tracking (<em>Yeh et al., 2021</em>) and global-scale weather forecasting (<em>Rasp et al., 2020; Bi et al., 2023</em>). These efforts increasingly rely on data-driven models, which require large volumes of curated, structured, and accessible EO data (<em>Reichstein et al., 2019</em>). However, preparing AI-ready EO datasets continues to be a significant challenge (<em>Sambasivan et al., 2021</em>; <em>Francis &amp; Czerkawski, 2024</em>). Most datasets require extensive preprocessing and reformatting before being integrated into AI pipelines, and only a small fraction are usable <code>out of the box</code>. Although AI-ready EO datasets have grown substantially, with more than 500 now cataloged (<em>Schmitt et al., 2023</em>), they still lack a unified structure and consistent metadata conventions. This fragmentation hinders reproducibility, limits interoperability, and slows the development of AI (<em>Dimitrovski et al., 2023</em>; <em>Long et al., 2021</em>). These issues are especially critical for training foundation models, which rely on combining diverse sources (<em>Marsocci et al., 2024</em>).</p><p>Insights from scientific communities can guide the development of standardised, AI-ready EO datasets. Fields such as climate science and geographic information systems (GIS) have long struggled with data standardisation and provide valuable lessons through widely adopted formats like NetCDF (<em>Treinish &amp; Gough, 1987</em>; <em>Rew &amp; Davis, 1990</em>) and GeoTIFF (<em>Ritter &amp; Ruth, 1997</em>; <em>Devys et al., 2019</em>). NetCDF was initially created as a binary format for multi-dimensional scientific data, with applications in atmospheric and climate sciences, but not limited to it. With broader use in climate science, it became evident that the standard NetCDF specification required complementary conventions to adequately represent the metadata needs of the domain. This realization led to the development of several metadata conventions, most notably the CF (Climate and Forecast) Conventions (<em>Eaton et al., 2024</em>), aimed at standardising the description of scientific variables, coordinates, and attributes. Although these conventions significantly improved interoperability, their <strong><code>text-based definitions</code></strong> introduced ambiguities and made consistent implementation difficult. To address this, formal data models, such as the CF data model (<em>Hassell et al., 2017</em>), were introduced years later, offering a structured and unambiguous interpretation of what CF-compliant data means. GeoTIFF, in contrast, took a more pragmatic approach. Designed to facilitate the exchange of raster data between GIS applications (<em>Ritter &amp; Ruth, 1997</em>), GeoTIFF embeds minimal but critical metadata, specifically the coordinate reference system (CRS) and geotransform, directly within the file (<em>Devys et al., 2019</em>). GeoTIFF, unlike NetCDF, was not developed with a comprehensive semantic model in mind. However, its simplicity and user-friendly design have led to widespread adoption.</p><p>In hindsight, both cases underscore the importance of maintainability. Crucially, both NetCDF and GeoTIFF have survived because active communities emerged around them, building tools and practices that reinforced and extended the specifications over time (<em>Devys et al., 2019</em>; <em>Maso et al., 2023</em>; <em>Eaton et al., 2024</em>). For CF-compliant NetCDF datasets, the experience highlighted the limitations of relying only on text-based definitions: as the authors of the CF data model argue in their conclusion, <code>creating an explicit data model before the CF conventions were written would arguably have been preferable. A data model encourages coherent implementations, which could be file storage syntaxes or software codes</code> (<em>Hassell et al., 2017</em>). In contrast, GeoTIFF illustrates how a well-defined minimal standard focused on a specific use case can achieve broad interoperability without necessitating a complex data model. These lessons highlight the need to balance formal rigor with practical simplicity. Given the inherent complexity of AI-ready EO datasets, we strongly support the development of a formal data model; however, whenever possible, it should be designed around the tools and workflows practitioners use on a daily basis to facilitate smooth adoption.</p><p>The FAIR principles (<em>Wilkinson et al., 2016</em>), Findability, Accessibility, Interoperability, and Reusability, provide a useful framework to address the challenges faced by the AI-ready EO datasets. Regarding <strong>Findability</strong>, web standardised metadata schemas (i.e., Schema.org, <em>Guha et al. 2016</em>) are rarely used to describe AI-ready EO datasets, limiting their visibility in search engines and data catalogs (<em>Benjelloun et al., 2024</em>). In terms of <strong>Accessibility</strong>, data access often depends on manual downloads or custom APIs rather than scalable, cloud-native formats that support partial or selective retrieval. With respect to <strong>Interoperability</strong>, the wide variety of formats, with differing conventions for byte layout, chunking strategies, compression, and explicit metadata, creates barriers to seamless integration across datasets. Finally, on <strong>Reusability</strong>, many datasets lack clear licenses, provenance, or documentation, making them difficult to audit, cite, or extend.</p><p>To close these gaps, we propose TACO (Transparent Access to Cloud Optimized Datasets), a FAIR-compliant, cloud-optimized specification for organizing AI-ready EO datasets. TACO files are self-contained, portable, and complete, encapsulating all the information required for sample interpretation without relying on external files or software dependencies. Built on widely supported technologies like GDAL and Apache Parquet, TACO allows for seamless integration across multiple programming languages.</p><p> </p><a name="fig1"><p align="center"><img src="https://github.com/user-attachments/assets/17a84407-0cd1-4da4-9e0d-2e22beda7087" width="50%"></p></a><sub><strong>Figure 1:</strong> Conceptual organization of the TACO Specification. The Data Model (A) is composed of two layers: Logical Structure (describing the relationships between data and metadata) and Semantic Description (standardised metadata definitions). These layers collectively define the Data Format (B), specifying how data is stored, which can be created and accessed through a dedicated API (C) consisting of the ToolBox (for creation) and the Reader (for reading).</sub><p> </p><h3 id="the-specification" tabindex="-1">The specification <a class="header-anchor" href="#the-specification" aria-label="Permalink to &quot;The specification&quot;">​</a></h3><p>The TACO specification defines the data model, file format, and API (<a href="#fig1"><strong>Figure 1</strong></a>). Here, the <strong><em>data model</em></strong> refers to an abstract representation of a dataset that defines the rules, constraints, and relationships connecting metadata to the associated data assets (<a href="#fig2"><strong>Figure 2</strong></a>). The <strong><em>data format</em></strong> defines the physical representation of the dataset, specifying how data and metadata are encoded, stored, and organized. Finally, the API specifies the programmatic methods and conventions by which users and applications can interact with TACO-compliant datasets. By providing a unique and well-structured interface, the API abstracts the underlying complexity of the data format and data model, allowing data users to query, modify, and even integrate multiple TACO datasets.</p><h4 id="the-data-model" tabindex="-1">The Data Model <a class="header-anchor" href="#the-data-model" aria-label="Permalink to &quot;The Data Model&quot;">​</a></h4><p>The logical structure of the TACO data model is illustrated in the UML diagram in <a href="#fig2"><strong>Figure 2</strong></a>. At its core, a TACO dataset is defined as a structured collection of minimal self-contained data units, called SAMPLEs, organized within a container, called TORTILLA, and enriched by dataset-level metadata.</p><p> </p><a name="fig2"><p align="center"><img src="https://github.com/user-attachments/assets/f41109aa-357f-4a2c-b348-e39192a9ccc6" alt="TACO logical structure" width="75%"></p></a><sub><strong>Figure 2:</strong> TACO logical structure. A <code>SAMPLE</code> encapsulates raw data and metadata, with a pointer to a <code>DataSource</code>. Supported data sources include <code>GDALDataset</code>, <code>BYTES</code>, and <code>TORTILLA</code>. TACO extends <code>TORTILLA</code> by adding high-level dataset metadata.</sub><p> </p><p>A SAMPLE represents the minimal self-contained and smallest indivisible unit for AI training and evaluation. Each SAMPLE encapsulates the actual data and metadata (<a href="#fig3"><strong>Figure 3</strong></a>). Importantly, each SAMPLE contains a pointer to a DataSource that specifies how to access the underlying data. A SAMPLE supports three primary DataSource types: (i) GDALDataset, for raster or vector data readable by the GDAL library; (ii) BYTES, representing raw byte streams for unsupported or custom formats; and (iii) TORTILLA. While the BYTES option is available, GDALDataset is recommended for partial read support.</p><p> </p><a name="fig3"></a><p align="center"><img src="https://github.com/user-attachments/assets/52dad4b8-d680-4f43-b666-23572e48df2e" alt="Semantic description of SAMPLE metadata" width="80%"></p><sub><strong>Figure 3:</strong> Semantic description of the <code>SAMPLE</code> metadata. The <code>Metadata</code> class contains essential file identification and storage fields. An abstract <code>Extension</code> class defines the interface for optional metadata, allowing for expansion. Specific extensions (marked with <code>&lt;&lt;Extension&gt;&gt;</code> in the header) like <code>STAC</code>, <code>RAI</code>, <code>STATS</code>, <code>Flood</code>, and <code>Methane</code> inherit from <code>Extension</code>, each adding domain-specific attributes. This design enables adding extensions without modifying the core <code>Metadata</code> structure.</sub><p> </p><p>The TORTILLA serves as a container that manages multiple SAMPLE instances. All SAMPLEs within a TORTILLA share a uniform metadata schema, enabling the combined metadata to be represented as a dataframe. Since TORTILLA implements the DataSource interface, it can be referenced within a SAMPLE, enabling recursive nesting of TORTILLA containers. This design supports the representation of hierarchical datasets while preserving the modularity and self-contained nature of individual SAMPLEs. Building upon TORTILLA, the TACO class extends this container structure by adding comprehensive dataset-level metadata (<a href="#fig4"><strong>Figure 4</strong></a>). This additional metadata provides a semantic collection overview, supporting dataset management, discovery, and interoperability.</p><p> </p><a name="fig4"></a><p align="center"><img src="https://github.com/user-attachments/assets/e522fc11-7cc1-4670-a836-3491c1c2b1c2" alt="Semantic description of SAMPLE metadata" width="80%"></p><sub><strong>Figure 4:</strong> Semantic description of the TACO dataset-level metadata. Core dataset information is structured in the Metadata class, linking mandatory and optional fields. Extensions, modeled through the abstract Extension class, allow modular inclusion of additional metadata such as RAI, Publications, and Sensor information, ensuring flexibility and scalability.</sub><p> </p><h4 id="semantic-description" tabindex="-1">Semantic Description <a class="header-anchor" href="#semantic-description" aria-label="Permalink to &quot;Semantic Description&quot;">​</a></h4><p>This section defines the structure of the metadata associated with each individual SAMPLE (<a href="#fig3"><strong>Figure 3</strong></a>) and with the TACO dataset (<a href="#fig4"><strong>Figure 4</strong></a>) as a whole. Metadata is organized into three categories: (1) Core (required fields), (2) Optional (non-essential fields providing additional context or supporting specific functionalities), and (3) Automatic (fields automatically generated by the TACO API; generation is based exclusively on core metadata and never on optional fields).</p><p> </p><a name="tab1"></a><table><thead><tr><th><strong>Field</strong></th><th><strong>Type</strong></th><th><strong>Details</strong></th></tr></thead><tbody><tr><td><code>tortilla:id</code></td><td>String</td><td><strong>CORE</strong>. Unique identifier for each item.</td></tr><tr><td><code>tortilla:file_format</code></td><td>String</td><td><strong>CORE</strong>. The format name <strong>MUST</strong> follow the GDAL naming convention. For example: <ul><li>GeoTIFF files use the format name <code>GTiff</code>.</li><li>JPEG files use the format name <code>JPEG</code>.</li></ul><strong>Additional Supported Formats:</strong><ul><li><code>BYTES</code>: Used for data formats not supported by GDAL.</li><li><code>TORTILLA</code>: Used when the file represents a nested TORTILLA structure.</li></ul></td></tr><tr><td><code>tortilla:offset</code></td><td>Long</td><td><strong>AUTOMATIC</strong>. Byte offset where the item’s data begins in the file. This field is automatically generated by the <code>taco-toolbox</code>.</td></tr><tr><td><code>tortilla:length</code></td><td>Long</td><td><strong>AUTOMATIC</strong>. Number of bytes that the item’s data occupies. This field is automatically generated by the <code>taco-toolbox</code>.</td></tr><tr><td><code>tortilla:data_split</code></td><td>String</td><td><strong>OPTIONAL</strong>. The data split type. <strong>MUST</strong> be one of the following: <ul><li><code>train</code>: Training data.</li><li><code>test</code>: Testing data.</li><li><code>validation</code>: Validation data.</li></ul></td></tr></tbody></table><p><strong>Table 1:</strong> Core Schema for <code>SAMPLE</code> Metadata</p><p> </p><p>At the <code>SAMPLE</code> level, two core attributes are required: <code>tortilla:id</code>, a unique string that identifies each <code>SAMPLE</code>, and <code>tortilla:file_format</code>, which specifies the data format—either <code>TORTILLA</code>, <code>BYTES</code>, or any format supported by GDAL. An optional field, <code>tortilla:data_split</code>, indicates the dataset partition to which the sample belongs (e.g., training, validation, or testing). Additionally, the fields <code>tortilla:offset</code> (denoting the position within a TORTILLA archive) and <code>tortilla:length</code> (the sample&#39;s size) are automatically computed by the TACO API (<a href="#tab1"><strong>Table 1</strong></a>). The current specification supports three optional extensions: STAC, Responsible AI (RAI), and sample statistics (STATS), which are described in detail in the <a href="#sample-level-extension"><code>SAMPLE</code> Extensions section</a>.</p><p>At the dataset level, TACO defines a <code>Metadata</code> class that encapsulates both core and optional fields describing the dataset’s provenance, structure, and content (<a href="#tab2"><strong>Table 2</strong></a>). Core fields include a persistent identifier (<code>id</code>), versioning information (<code>taco_version</code>, <code>dataset_version</code>), spatiotemporal coverage (<code>extent</code>), a human-readable description (<code>description</code>), licensing details (<code>licenses</code>), and contact information for both dataset providers (<code>providers</code>) and the individual responsible for converting the data into TACO (<code>data_curator</code>). Several of these core fields employ nested structures or lists to represent complex information. For example, both <code>providers</code> and <code>data_curator</code> are modeled as lists of <code>Contact</code> objects, each containing attributes such as name, affiliation, and email. The <code>extent</code> field uses nested list structures to capture spatial and temporal bounds, while the <code>licenses</code> field is represented by a <code>Licenses</code> class that can wrap one or more license entries.</p><p>Optional fields in the <code>Metadata</code> class include a dataset title, descriptive keywords, and high-level information about intended use, such as the task type and split strategy. Links to external resources can be provided via the optional <code>raw_link</code> and <code>discuss_link</code> fields, both represented by a <code>Hyperlink</code> class that includes an <code>href</code> and a textual <code>description</code>. TACO metadata is designed to be extensible: additional modules can be integrated by inheriting from an abstract <code>Extension</code> class. Check the <a href="#taco-level-extension"><code>TACO</code> Extensions section</a> for more details.</p><p> </p><table tabindex="0"><thead><tr><th><strong>Field</strong></th><th><strong>Type</strong></th><th><strong>Details</strong></th></tr></thead><tbody><tr><td><code>id</code></td><td>String</td><td><strong>CORE</strong>. A unique identifier for the dataset.</td></tr><tr><td><code>taco_version</code></td><td>String</td><td><strong>CORE</strong>. The version of the TACO specification.</td></tr><tr><td><code>dataset_version</code></td><td>String</td><td><strong>CORE</strong>. Version of the dataset.</td></tr><tr><td><code>description</code></td><td>String</td><td><strong>CORE</strong>. Description of the dataset.</td></tr><tr><td><code>licenses</code></td><td>List of strings</td><td><strong>CORE</strong>. License(s) of the dataset. It is recommended to use <a href="https://spdx.org/licenses/" target="_blank" rel="noreferrer">SPDX License identifiers</a>.</td></tr><tr><td><code>extent</code></td><td><a href="#extent-object">Extent Object</a></td><td><strong>CORE</strong>. Spatial and temporal extents.</td></tr><tr><td><code>providers</code></td><td>List of <a href="#person-object">Person Objects</a></td><td><strong>CORE</strong>. A list of persons who participated in the creation of the dataset.</td></tr><tr><td><code>curators</code></td><td>List of <a href="#person-object">Person Objects</a></td><td><strong>CORE</strong>. A list of persons responsible for converting the dataset to TACO compliance.</td></tr><tr><td><code>title</code></td><td>String</td><td><strong>OPTIONAL</strong>. Title of the dataset. Maximum length: 250 characters.</td></tr><tr><td><code>keywords</code></td><td>List of strings</td><td><strong>OPTIONAL</strong>. List of keywords describing the dataset.</td></tr><tr><td><code>task</code></td><td><a href="#task-extension">Task Object</a></td><td><strong>OPTIONAL</strong>. Refers to the most relevant task defined by the TACO specification.</td></tr><tr><td><code>split_strategy</code></td><td><a href="#split-strategy-extension">Split Strategy Object</a></td><td><strong>OPTIONAL</strong>. Chosen from an explicit list of method names.</td></tr><tr><td><code>discuss_link</code></td><td><a href="#hyperlink-object">HyperLink Object</a></td><td><strong>OPTIONAL</strong>. A link to a discussion forum or community page.</td></tr><tr><td><code>raw_link</code></td><td><a href="#hyperlink-object">HyperLink Object</a></td><td><strong>OPTIONAL</strong>. Link to the raw dataset (if not in native TACO format).</td></tr></tbody></table><p> </p><h4 id="data-format" tabindex="-1">Data format <a class="header-anchor" href="#data-format" aria-label="Permalink to &quot;Data format&quot;">​</a></h4><p>The <strong>TORTILLA</strong> and <strong>TACO</strong> file formats are designed to efficiently store large-scale datasets using a binary serialization scheme (<a href="#fig5"><strong>Figure 5</strong></a>). Each TORTILLA file enforces a consistent schema and metadata structure across all its samples. Metadata is stored in the <strong>FOOTER</strong> using Apache Parquet, while the corresponding sample data is stored as a Binary Large Object (<strong>BLOB</strong>). Each row in the Apache Parquet file corresponds to a distinct <code>SAMPLE</code> object. The <strong>BLOB</strong> and the <strong>FOOTER</strong> are combined into a single file, constituting the TORTILLA format (see <a href="#fig5"><strong>Figure 5</strong></a>). Notably, the format enables partial reads of the <strong>BLOB</strong> during sample-level access, while the <strong>FOOTER</strong> is read entirely only once at load time.</p><p> </p><a name="fig5"></a><p align="center"><img src="https://github.com/user-attachments/assets/47cf46be-6f40-4aec-bf97-9a674021bcfa" alt="Semantic description of SAMPLE metadata" width="80%"></p><sub><strong>Figure 5:</strong> Structure of the TACO and TORTILLA file format, used as the underlying container for SAMPLEs. The format consists of a 200-byte static header followed by a dynamic segment. The static section encodes file-level metadata including a magic number (MB), footer offset (FO) and length (FL), data partition (DP), and pointers to the metadata collection (CO and CL, only for TACO). The dynamic section serializes data blobs (DATA), sample-level descriptors (FOOTER), and, in the case of TACO files only, a dataset-level metadata block (COLLECTION) encoded in UTF-8 JSON.</sub><p> </p><p>A <strong>TACO</strong> file extends the TORTILLA format by appending dataset-level metadata (the <strong>COLLECTION</strong>), encoded in JSON at the end of the file. This design ensures that both TORTILLA and TACO files are self-contained, portable, and complete, encapsulating all information required to interpret samples without reliance on external files or software dependencies.</p><p>Each file begins with a fixed 200-byte <strong>HEADER</strong> that includes a 2-byte magic number, an 8-byte offset and length for the <strong>FOOTER</strong>, and an 8-byte data partition count indicating the dataset&#39;s number of segments. This count allows the TACO API to verify dataset completeness and reconstruct the full archive correctly. TACO files introduce two additional 8-byte fields for the <strong>COLLECTION</strong> offset and length. Both formats reserve unused space in the header for future use: 174 bytes in TORTILLA and 158 bytes in TACO.</p><p>The TACO API (Section <a href="#api"><strong>API</strong></a>) automatically generates certain fields based on the input data. For instance, it records sample-level offsets and lengths as columns in the <strong>FOOTER</strong>, enabling efficient random access to individual samples (illustrated by the red dotted line in <a href="#fig5"><strong>Figure 5</strong></a>). To support multi-language interoperability and partial reads, TACO relies on GDAL’s Virtual File System (VFS), particularly the <code>/vsisubfile/</code> handler, which allows byte ranges within a TACO file to be treated as standalone <code>GDALDataset</code> objects. This enables fast random access without reading the entire <strong>BLOB</strong> region. TACO also supports cloud-optimized access, leveraging additional GDAL VFS handlers such as <code>/vsicurl/</code>, <code>/vsis3/</code>, <code>/vsiaz/</code>, <code>/vsigs/</code>, <code>/vsioss/</code>, and <code>/vsiswift/</code>, ensuring high-performance reads across diverse cloud storage platforms.</p><p> </p><a name="fig6"></a><p align="center"><img src="https://github.com/user-attachments/assets/ac910f8e-84d8-4b1a-bcbe-d46dba349692" width="80%"></p><sub><strong>Figure 6:</strong> This diagram illustrates the key components of the TACO Toolbox API and their relationships. The Toolbox is responsible for creating, editing, and mapping between standards.</sub><p> </p><h4 id="api" tabindex="-1">API <a class="header-anchor" href="#api" aria-label="Permalink to &quot;API&quot;">​</a></h4><p>The TACO API consists of two main components: the <strong>Toolbox</strong> (<a href="#fig6"><strong>Figure 6</strong></a>) and the <strong>Reader</strong> (<a href="#fig7"><strong>Figure 7</strong></a>). The Toolbox provides data classes for the core TACO models—<code>SAMPLE</code>, <code>TORTILLA</code>, and <code>TACO</code>—enabling users to define and modify dataset structures entirely through code. It includes a <code>create()</code> method that serializes both data and metadata into fully compliant TACO or TORTILLA files. Additionally, the <code>edit()</code> method allows users to update existing files, whether adjusting the <code>COLLECTION</code> or the <code>FOOTER</code>.</p><p>Format conversion is supported through optional utilities such as <code>tortilla2taco()</code>, <code>taco2tortilla()</code>, <code>footer2geoparquet()</code>, and <code>footer2geoparquetstac()</code>. Exporters like <code>collection2stac()</code>, <code>collection2croissant()</code>, <code>collection2datacite()</code>, and <code>collection2datacard()</code> enable collection-level metadata generation in STAC, Croissant, DataCite, or Markdown formats.</p><p>The <strong>Reader</strong> component provides a simple interface to load and interact with TACO and TORTILLA files. It implements a <code>load()</code> function that retrieves the <code>FOOTER</code> and, if called with <code>collection=True</code>, also returns the <code>COLLECTION</code>. A <code>compile()</code> function must also be provided to create smaller subsets of existing TACO or TORTILLA files.</p><p>The Reader is designed to operate within a DataFrame interface in the target programming language (e.g., R, Python, or Julia), mapping the <code>FOOTER</code> to a DataFrame object. Additionally, a <code>read</code> method must be implemented on the DataFrame to expose GDAL VFS access (Figure~\ref{fig:api_reader}). Optional helper functions can also be included to perform sanity checks and validate file compliance with the TACO format specification.</p><p> </p><a name="fig7"></a><p align="center"><img src="https://github.com/user-attachments/assets/ac910f8e-84d8-4b1a-bcbe-d46dba349692" width="80%"></p><sub><strong>Figure 7:</strong> Overview of the TACO Reader API. This diagram illustrates the core components and their interactions. The Reader parses the FOOTER of TACO and TORTILLA objects and converts them into a DataFrame. Individual SAMPLEs can then be accessed using the read method, which enables sample-level querying and downstream analysis.</sub><p> </p><h2 id="extensions" tabindex="-1">Extensions <a class="header-anchor" href="#extensions" aria-label="Permalink to &quot;Extensions&quot;">​</a></h2><h3 id="sample-level-semantic-description" tabindex="-1">Sample-level Semantic Description <a class="header-anchor" href="#sample-level-semantic-description" aria-label="Permalink to &quot;Sample-level Semantic Description&quot;">​</a></h3><h4 id="stac-extension" tabindex="-1">STAC extension <a class="header-anchor" href="#stac-extension" aria-label="Permalink to &quot;STAC extension&quot;">​</a></h4><p>This section describes the integration of SpatioTemporal Asset Catalog (STAC) metadata at the item level, where each <code>SAMPLE</code> corresponds to a STAC Item. STAC provides a standardized schema for spatially and temporally contextualizing assets. Although our schema does not adopt the exact naming conventions defined in official STAC, the current <code>SAMPLE</code> STAC extension allows for a direct mapping between the two specifications.</p><table><thead><tr><th><strong>Field</strong></th><th><strong>Type</strong></th><th><strong>Details</strong></th></tr></thead><tbody><tr><td><code>stac:crs</code></td><td>String</td><td><strong>CORE</strong>. The Coordinate Reference System (CRS), specified using a recognized authority (EPSG, ESRI or SR-ORG).</td></tr><tr><td><code>stac:geotransform</code></td><td>Array of Floats</td><td><strong>CORE</strong>. A 6-element array defining the affine transformation from pixel to spatial coordinates, following GDAL conventions: <ul><li><code>a</code>: Top-left x-coordinate of the upper-left pixel</li><li><code>b</code>: Pixel width (x-resolution)</li><li><code>c</code>: Row rotation (usually 0)</li><li><code>d</code>: Top-left y-coordinate of the upper-left pixel</li><li><code>e</code>: Column rotation (usually 0)</li><li><code>f</code>: Negative pixel height (y-resolution, negative for north-up)</li></ul></td></tr><tr><td><code>stac:tensor_shape</code></td><td>Array of integers</td><td><strong>CORE</strong>. The spatial dimensions of the sample.</td></tr><tr><td><code>stac:time_start</code></td><td>Integer</td><td><strong>CORE</strong>. Timestamp in seconds since UNIX epoch, representing the nominal start of acquisition.</td></tr><tr><td><code>stac:time_end</code></td><td>Integer</td><td><strong>CORE</strong>. Timestamp marking the end of the acquisition or composite period.</td></tr><tr><td><code>stac:centroid</code></td><td>String</td><td><strong>AUTOMATIC</strong>. Centroid of the sample in WKT <code>POINT</code> (EPSG:4326).</td></tr></tbody></table><h4 id="rai-extension" tabindex="-1">RAI extension <a class="header-anchor" href="#rai-extension" aria-label="Permalink to &quot;RAI extension&quot;">​</a></h4><p>The RAI (Responsible AI) extension automatically enriches each <code>SAMPLE</code> with socioeconomic and environmental indicators by spatially overlaying its footprint with global datasets.</p><table><thead><tr><th><strong>Field</strong></th><th><strong>Type</strong></th><th><strong>Details</strong></th></tr></thead><tbody><tr><td><code>rai:elevation</code></td><td>Long</td><td><strong>AUTOMATIC</strong>. Average elevation in meters within the Sample footprint (from <a href="https://doi.org/10.5069/G9028PQB">Copernicus DEM</a>).</td></tr><tr><td><code>rai:cisi</code></td><td>Float</td><td><strong>AUTOMATIC</strong>. Critical Infrastructure Spatial Index (0–1). See <a href="https://doi.org/10.1038/s41597-022-01218-4">doi:10.1038/s41597-022-01218-4</a>.</td></tr><tr><td><code>rai:gdp</code></td><td>Float</td><td><strong>AUTOMATIC</strong>. GDP (USD/year) averaged over footprint. See <a href="https://doi.org/10.1038/sdata.2018.4">doi:10.1038/sdata.2018.4</a>.</td></tr><tr><td><code>rai:hdi</code></td><td>Float</td><td><strong>AUTOMATIC</strong>. Human Development Index (0–1). See <a href="https://doi.org/10.1038/sdata.2018.4">doi:10.1038/sdata.2018.4</a>.</td></tr><tr><td><code>rai:gmi</code></td><td>Float</td><td><strong>AUTOMATIC</strong>. Global human modification index. See <a href="https://doi.org/10.5194/essd-12-1953-2020">doi:10.5194/essd-12-1953-2020</a>.</td></tr><tr><td><code>rai:pop</code></td><td>Float</td><td><strong>AUTOMATIC</strong>. Estimated population (LandScan). See <a href="https://doi.org/10.48690/1531770">doi:10.48690/1531770</a>.</td></tr><tr><td><code>rai:admin0</code></td><td>String</td><td><strong>AUTOMATIC</strong>. Country-level boundary. See <a href="https://doi.org/10.1371/journal.pone.0231866">doi:10.1371/journal.pone.0231866</a>.</td></tr><tr><td><code>rai:admin1</code></td><td>String</td><td><strong>AUTOMATIC</strong>. District-level boundary. Same source as above.</td></tr><tr><td><code>rai:admin2</code></td><td>String</td><td><strong>AUTOMATIC</strong>. Municipality-level boundary. Same source as above.</td></tr></tbody></table><h4 id="stats-extension" tabindex="-1">STATS extension <a class="header-anchor" href="#stats-extension" aria-label="Permalink to &quot;STATS extension&quot;">​</a></h4><p>The STATS extension provides descriptive statistics summarizing the pixel values of each <code>SAMPLE</code>. These statistics are computed automatically by the TACO API when the <code>file_format</code> is set to <code>Gtiff</code>, and they are calculated per band across the spatial dimensions (height × width) of the image. This extension defines four fields: <code>stats:mean</code>, <code>stats:min</code>, <code>stats:max</code>, and <code>stats:std</code>. Each field is represented as an array of scalars, with one value per channel. These statistics are essential for tasks such as input normalization, quality assessment, and characterization of value distributions across heterogeneous datasets. Importantly, when all samples in a TORTILLA archive include STATS metadata, the TACO API enables users to compute global or subset-level statistics through pooled variance and weighted averages, without requiring the entire dataset to be loaded into memory.</p><h4 id="stats-fields" tabindex="-1">STATS Fields <a class="header-anchor" href="#stats-fields" aria-label="Permalink to &quot;STATS Fields&quot;">​</a></h4><table tabindex="0"><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>stats:mean</code></td><td>Array of Floats</td><td><strong>AUTOMATIC</strong>. The mean value of each band, computed across the height × width spatial dimensions.</td></tr><tr><td><code>stats:min</code></td><td>Array of Floats</td><td><strong>AUTOMATIC</strong>. The minimum value of each band across the image.</td></tr><tr><td><code>stats:max</code></td><td>Array of Floats</td><td><strong>AUTOMATIC</strong>. The maximum value of each band across the image.</td></tr><tr><td><code>stats:std</code></td><td>Array of Floats</td><td><strong>AUTOMATIC</strong>. The standard deviation of each band.</td></tr></tbody></table><h3 id="taco-level-semantic-description" tabindex="-1">TACO-level Semantic Description <a class="header-anchor" href="#taco-level-semantic-description" aria-label="Permalink to &quot;TACO-level Semantic Description&quot;">​</a></h3><h4 id="extent-object" tabindex="-1">Extent object <a class="header-anchor" href="#extent-object" aria-label="Permalink to &quot;Extent object&quot;">​</a></h4><p>Describes the spatial and temporal coverage of the entire dataset. Both spatial and temporal extents are required.</p><table tabindex="0"><thead><tr><th><strong>Field</strong></th><th><strong>Type</strong></th><th><strong>Details</strong></th></tr></thead><tbody><tr><td><code>spatial</code></td><td>List of numbers</td><td><strong>CORE</strong>. Bounding box defined as <code>[xmin, ymin, xmax, ymax]</code> in EPSG:4326.</td></tr><tr><td><code>temporal</code></td><td>List of integers</td><td><strong>CORE</strong>. Start and end dates in milliseconds since Unix Epoch (Jan 1, 1970, UTC).</td></tr></tbody></table><h4 id="person-object" tabindex="-1">Person object <a class="header-anchor" href="#person-object" aria-label="Permalink to &quot;Person object&quot;">​</a></h4><p>The <strong>Person object</strong> is based on the <a href="https://github.com/stac-extensions/contacts" target="_blank" rel="noreferrer">STAC Extension</a> proposed by Matthias Mohr. It identifies and provides contact details for a person or organization responsible for a resource.</p><table tabindex="0"><thead><tr><th><strong>Field</strong></th><th><strong>Type</strong></th><th><strong>Details</strong></th></tr></thead><tbody><tr><td><code>name</code></td><td>String</td><td><strong>CORE</strong>. Name of the responsible person (if <code>organization</code> is missing).</td></tr><tr><td><code>organization</code></td><td>String</td><td><strong>OPTIONAL</strong>. Affiliation of the contact (if <code>name</code> is missing).</td></tr><tr><td><code>emails</code></td><td>List of Info Objects</td><td><strong>OPTIONAL</strong>. Optional email addresses.</td></tr><tr><td><code>roles</code></td><td>List of strings</td><td><strong>OPTIONAL</strong>. Optional roles (duties, functions, permissions) associated with this contact.</td></tr></tbody></table><h4 id="hyperlink-object" tabindex="-1">Hyperlink Object <a class="header-anchor" href="#hyperlink-object" aria-label="Permalink to &quot;Hyperlink Object&quot;">​</a></h4><p>The Hyperlink class defines a URL and its associated description. The URL must follow <a href="https://www.rfc-editor.org/rfc/rfc3986" target="_blank" rel="noreferrer">RFC 3986</a> standards.</p><table tabindex="0"><thead><tr><th><strong>Field</strong></th><th><strong>Type</strong></th><th><strong>Details</strong></th></tr></thead><tbody><tr><td><code>href</code></td><td>String</td><td><strong>CORE</strong>. URL of the resource. Must be a valid URI (RFC 3986).</td></tr><tr><td><code>description</code></td><td>String</td><td><strong>OPTIONAL</strong>. Optional explanation or context for the hyperlink.</td></tr></tbody></table><h4 id="task-extension" tabindex="-1">Task Extension <a class="header-anchor" href="#task-extension" aria-label="Permalink to &quot;Task Extension&quot;">​</a></h4><p>The <code>task</code> field must be a string selected from a well-defined and consistent list of supported ML tasks. It defines the primary ML task that the dataset supports.</p><table tabindex="0"><thead><tr><th><strong>Field</strong></th><th><strong>Type</strong></th><th><strong>Details</strong></th></tr></thead><tbody><tr><td><code>task</code></td><td>String (Literal)</td><td><strong>CORE</strong>. Type of machine learning task.</td></tr></tbody></table><p>The task field must be one of the following values:</p><ul><li><strong>Regression</strong>: Estimates a numeric and continuous value.</li><li><strong>Classification</strong>: Assigns predefined class labels to an output.</li><li><strong>Scene Classification</strong>: Assigns a single class label to an entire scene or area.</li><li><strong>Object Detection</strong>: Identifies and localizes objects using bounding boxes.</li><li><strong>Segmentation</strong>: Labels individual pixels in an image.</li><li><strong>Semantic Segmentation</strong>: Pixel-wise classification without object differentiation.</li><li><strong>Instance Segmentation</strong>: Labels each distinct object at the pixel level.</li><li><strong>Panoptic Segmentation</strong>: Merges semantic and instance segmentation.</li><li><strong>Similarity Search</strong>: Checks if a query matches any reference item.</li><li><strong>Generative</strong>: Produces synthetic data.</li><li><strong>Image Captioning</strong>: Generates textual descriptions of images.</li><li><strong>Super Resolution</strong>: Enhances image resolution and detail.</li><li><strong>Denoising</strong>: Removes noise artifacts.</li><li><strong>Inpainting</strong>: Reconstructs missing/corrupt regions.</li><li><strong>Colorization</strong>: Adds color to grayscale images.</li><li><strong>Style Transfer</strong>: Transfers style from one image to another.</li><li><strong>Deblurring</strong>: Removes blur from an image.</li><li><strong>Dehazing</strong>: Removes haze/fog to enhance clarity.</li><li><strong>General</strong>: Use only if no specific task applies; clarify as needed.</li></ul><h4 id="split-strategy-extension" tabindex="-1">Split Strategy Extension <a class="header-anchor" href="#split-strategy-extension" aria-label="Permalink to &quot;Split Strategy Extension&quot;">​</a></h4><p>The core <code>split_strategy</code> field is a string that <strong>must</strong> be chosen from a predefined list of supported splitting approaches. This field details how the dataset is partitioned into distinct subsets, typically for training, validation, and testing machine learning models.</p><table tabindex="0"><thead><tr><th><strong>Field</strong></th><th><strong>Type</strong></th><th><strong>Details</strong></th></tr></thead><tbody><tr><td><code>split_strategy</code></td><td>String (Literal)</td><td><strong>CORE</strong>. The method used to split the dataset.</td></tr></tbody></table><p><strong>Supported <code>split_strategy</code> values:</strong></p><ul><li><strong>random</strong>: The dataset is split into training, validation, and testing subsets through a randomized process.</li><li><strong>stratified</strong>: The dataset is split while preserving the distribution of a specific property, such as temporal periods (e.g., splitting by year or season) or spatial characteristics (e.g., by geographic location).</li><li><strong>other</strong>: The dataset is split using a custom or non-standard method. Additional description is recommended.</li><li><strong>none</strong>: The dataset is not explicitly divided into subsets.</li><li><strong>unknown</strong>: The method used to split the dataset is not known or unspecified.</li></ul><h4 id="sensor-extension" tabindex="-1">Sensor Extension <a class="header-anchor" href="#sensor-extension" aria-label="Permalink to &quot;Sensor Extension&quot;">​</a></h4><p>The Sensor extension provides information about the optical remote sensing data, including the sensor used and the spectral bands available. Users can specify the sensor name (e.g., <code>landsat8oli</code>, <code>sentinel2msi</code>) and optionally select a subset of bands (e.g., <code>landsat8oli[B01, B02]</code>). If recognized, the TACO API automatically populates the corresponding bands.</p><table tabindex="0"><thead><tr><th><strong>Field</strong></th><th><strong>Type</strong></th><th><strong>Details</strong></th></tr></thead><tbody><tr><td><code>sensor</code></td><td>String</td><td><strong>CORE</strong>. The sensor that acquired the data (optional). <strong>Supported sensors:</strong> <code>landsat1mss</code>, <code>landsat2mss</code>, <code>landsat3mss</code>, <code>landsat4mss</code>, <code>landsat5mss</code>, <code>landsat4tm</code>, <code>landsat5tm</code>, <code>landsat7etm</code>, <code>landsat8oli</code>, <code>landsat9oli</code>, <code>sentinel2msi</code>, <code>eo1ali</code>, <code>aster</code>, <code>modis</code></td></tr><tr><td><code>bands</code></td><td>List of <a href="#spectral-band-extension">Spectral Band</a> objects</td><td><strong>OPTIONAL</strong>. A list of spectral band objects. If not provided directly, it will be inferred from the <code>sensor</code> field if recognized.</td></tr></tbody></table><h4 id="spectral-band-extension" tabindex="-1">Spectral Band Extension <a class="header-anchor" href="#spectral-band-extension" aria-label="Permalink to &quot;Spectral Band Extension&quot;">​</a></h4><p>The spectral band extension describes characteristics of individual bands associated with a given sensor.</p><table tabindex="0"><thead><tr><th><strong>Field</strong></th><th><strong>Type</strong></th><th><strong>Details</strong></th></tr></thead><tbody><tr><td><code>name</code></td><td>String</td><td>Unique name of the band (e.g., &quot;B02&quot;, &quot;red&quot;) (<strong>required</strong>)</td></tr><tr><td><code>index</code></td><td>Integer</td><td>Index of the band</td></tr><tr><td><code>common_name</code></td><td>String</td><td>Common name (e.g., &quot;blue&quot;, &quot;green&quot;) (optional)</td></tr><tr><td><code>description</code></td><td>String</td><td>Description of the band (optional)</td></tr><tr><td><code>unit</code></td><td>String</td><td>Unit of measurement (optional)</td></tr><tr><td><code>center_wavelength</code></td><td>Float</td><td>Central wavelength of the band (optional)</td></tr><tr><td><code>full_width_half_max</code></td><td>Float</td><td>Full width at half maximum (FWHM), a measure of spectral resolution (optional)</td></tr></tbody></table><h4 id="label-extension" tabindex="-1">Label Extension <a class="header-anchor" href="#label-extension" aria-label="Permalink to &quot;Label Extension&quot;">​</a></h4><p>The Label extension defines label data in a dataset. A <code>Label</code> object includes a list of <code>LabelClass</code> objects.</p><table tabindex="0"><thead><tr><th><strong>Field</strong></th><th><strong>Type</strong></th><th><strong>Details</strong></th></tr></thead><tbody><tr><td><code>label_classes</code></td><td>List of <a href="#label-class-extension">Label Class</a> Objects</td><td>A list where each element defines a label class (<strong>required</strong>)</td></tr><tr><td><code>label_description</code></td><td>String</td><td>An optional description of the labels used</td></tr></tbody></table><h4 id="label-class-extension" tabindex="-1">Label Class Extension <a class="header-anchor" href="#label-class-extension" aria-label="Permalink to &quot;Label Class Extension&quot;">​</a></h4><p>Each <code>LabelClass</code> defines a specific category or class in the dataset.</p><table tabindex="0"><thead><tr><th><strong>Field</strong></th><th><strong>Type</strong></th><th><strong>Details</strong></th></tr></thead><tbody><tr><td><code>name</code></td><td>String</td><td>Unique human-readable name (e.g., &quot;car&quot;, &quot;building&quot;) (<strong>required</strong>)</td></tr><tr><td><code>category</code></td><td>String or Integer</td><td>A broader category the label belongs to (<strong>required</strong>)</td></tr><tr><td><code>description</code></td><td>String</td><td>Optional detailed description</td></tr></tbody></table><h4 id="scientific-extension" tabindex="-1">Scientific Extension <a class="header-anchor" href="#scientific-extension" aria-label="Permalink to &quot;Scientific Extension&quot;">​</a></h4><p>This extension standardizes links to related scientific publications. The TACO scientific extension is based on the <a href="https://github.com/stac-extensions/scientific" target="_blank" rel="noreferrer">STAC Scientific Extension</a>.</p><table tabindex="0"><thead><tr><th><strong>Field</strong></th><th><strong>Type</strong></th><th><strong>Details</strong></th></tr></thead><tbody><tr><td><code>doi</code></td><td>String</td><td>Digital Object Identifier (DOI) of the dataset</td></tr><tr><td><code>citation</code></td><td>String</td><td>Full BibTeX citation</td></tr><tr><td><code>summary</code></td><td>String</td><td>Brief dataset summary</td></tr><tr><td><code>publications</code></td><td>List of <a href="#publication-extension">Publication</a> Objects</td><td>A list of related scientific works, conforming to the <code>Publication Object</code> specification</td></tr></tbody></table><h4 id="publication-extension" tabindex="-1">Publication Extension <a class="header-anchor" href="#publication-extension" aria-label="Permalink to &quot;Publication Extension&quot;">​</a></h4><p>The <code>Publication</code> object contains metadata for a scientific publication related to the dataset.</p><table tabindex="0"><thead><tr><th><strong>Field</strong></th><th><strong>Type</strong></th><th><strong>Details</strong></th></tr></thead><tbody><tr><td><code>doi</code></td><td>String</td><td>DOI of the publication (<strong>required</strong>)</td></tr><tr><td><code>citation</code></td><td>String</td><td>Full BibTeX citation (<strong>required</strong>)</td></tr><tr><td><code>summary</code></td><td>String</td><td>Summary or abstract of the publication (<strong>required</strong>)</td></tr></tbody></table><h3 id="facilitating-dataset-streaming-with-togs" tabindex="-1">Facilitating dataset streaming with TOGs <a class="header-anchor" href="#facilitating-dataset-streaming-with-togs" aria-label="Permalink to &quot;Facilitating dataset streaming with TOGs&quot;">​</a></h3><p>Since version 0.0.2, TACO supports fully streamable datasets, offering significant benefits for nested datasets. These streaming capabilities eliminate the need to copy the whole dataset to a local disk by enabling on-demand reading, which improves performance in cloud-based workflows. This mechanism substantially reduces the HTTP GET requests typically incurred when accessing individual samples. In non-streaming workflows, inspecting each sample within a nested dataset leads to a separate Parquet read per sample. This translates into multiple HTTP requests on cloud infrastructure, increasing both latency and operational cost.</p><p> </p><a name="fig7"></a><p align="center"><img src="https://github.com/user-attachments/assets/a895b1b0-093e-4e5a-b6ca-5dd40d9d8112" width="80%"></p><sub><strong>Figure 8:</strong>Structure of the streamable TACO file format. Unlike the conventional TACO, the streamable variant explicitly encodes byte-level `offset` and `length` values for each data sample, enabling efficient random access to nested content. In this example, the format supports hierarchical access to `TORTILLA` samples and their associated GeoTIFF components. Metadata is organized across multiple Parquet tables, each exposing the necessary layout information for deserialization. The final table encodes fine-grained chunking (e.g., `lrf::0`, `lrf::1`), supporting selective retrieval of individual nD array blocks.</sub><p> </p><p>To address this, TACO now embeds all metadata (including nested metadata) directly into the FOOTER dataset, including top-level sample metadata and fine-grained chunking details. As illustrated in Figure 8, the footer is organized as a sequence of <em>n + 2</em> Parquet objects, where <em>n</em> is the nesting depth (with <em>n = 0</em> for non-nested datasets). Regardless of the specific case, two Parquet objects MUST always included:</p><ul><li>A <strong>top-level</strong> Parquet file containing sample-level metadata.</li><li>A <strong>bottom-level</strong> Parquet file containing chunking-level metadata.</li></ul><p>The TACO file HEADER stores the byte offset and length only of the first Parquet object in the FOOTER. Each Parquet object includes object-level metadata as a JSON string under the key <strong><code>pointer</code></strong>, formatted as <code>OFFSETLENGTH(A, B)</code>, where <em>A</em> and <em>B</em> are the byte offset and length of the next Parquet object. This chain continues until a <strong><code>pointer</code></strong> value of <em>NULL</em> indicates the end of the metadata sequence. Additionally, all Parquet objects in the FOOTER include a column named <code>tortilla:root</code>, which acts as a primary key to preserve the relational structure across dataset levels.</p><h4 id="hierarchical-data-access-and-return-types-at-each-level" tabindex="-1">Hierarchical data access and return types at each level <a class="header-anchor" href="#hierarchical-data-access-and-return-types-at-each-level" aria-label="Permalink to &quot;Hierarchical data access and return types at each level&quot;">​</a></h4><p>The hierarchical design of the streamable TACO format enables intuitive and efficient data access. The <code>.read()</code> method at each level returns an object corresponding to that level&#39;s data structure, facilitating smooth traversal through nested datasets:</p><ul><li><p><strong>Reading a Sample-Level with <code>TORTILLA</code> data object:</strong></p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">db.read(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;img1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div></li></ul><p>Returns a <strong>DataFrame object</strong> containing metadata for the sample <code>&quot;img1&quot;</code>. This DataFrame includes references to nested objects but does not load the nested content itself.</p><ul><li><p><strong>Reading a Sample-Level with <code>Gtiff</code> data object:</strong></p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">db.read(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;img1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).read(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;lrf&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div></li></ul><p>Returns a <strong>VFS (Virtual File System) GDAL string snippet</strong>, representing a handle to the GeoTIFF component <code>&quot;lrf&quot;</code> associated with the sample <code>&quot;img1&quot;</code>. This string can be passed directly to GDAL-compatible libraries for raster access without fully materializing the file.</p><ul><li><p><strong>Chunk-level read:</strong></p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">db.read(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;img1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).read(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;lrf::2&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div></li></ul><p>Returns a <a href="https://arrow.apache.org/docs/python/generated/pyarrow.Tensor.html" target="_blank" rel="noreferrer"><strong>PyArrow Tensor</strong></a> object corresponding to a fine-grained chunk (e.g., the third chunk) of the n-dimensional array within the <code>&quot;lrf&quot;</code> GeoTIFF. This progressive reading strategy optimizes I/O operations by avoiding unnecessary data loading, enabling scalable and efficient workflows in large, nested remote sensing datasets.</p><h4 id="taco-optimized-geotiff-tog-data-object" tabindex="-1">TACO Optimized GeoTIFF (TOG) data object <a class="header-anchor" href="#taco-optimized-geotiff-tog-data-object" aria-label="Permalink to &quot;TACO Optimized GeoTIFF (TOG) data object&quot;">​</a></h4><p>When using streaming mode, the <strong>data for each sample must be saved as a TACO Optimized GeoTIFF (TOG)</strong>. A TOG is a GDAL-compatible GeoTIFF file created with specific compression and tiling settings to support efficient, ML-friendly workflows.</p><table tabindex="0"><thead><tr><th>Parameter</th><th>Value</th><th>Notes</th></tr></thead><tbody><tr><td><code>driver</code></td><td><code>&quot;COG&quot;</code></td><td>Cloud-Optimized GeoTIFF format</td></tr><tr><td><code>interleave</code></td><td><code>&quot;tile&quot;</code></td><td>Ensures optimal layout for tiled reading</td></tr><tr><td><code>blocksize</code></td><td><code>n</code></td><td>Must be divisible by 16.</td></tr><tr><td><code>compress</code></td><td><code>&quot;zstd&quot;</code></td><td>Zstandard compression for fast and efficient storage</td></tr><tr><td><code>level</code></td><td><code>13</code></td><td>Compression level</td></tr><tr><td><code>bigtiff</code></td><td><code>&quot;yes&quot;</code></td><td>Enables support for files larger than 4 GiB</td></tr><tr><td><code>overview</code></td><td><code>&quot;none&quot;</code></td><td>No OVERVIEWS MUST be generated</td></tr><tr><td><code>predictor</code></td><td><code>&quot;yes&quot;</code></td><td>Enables predictor for better compression ratios</td></tr></tbody></table><h4 id="precomputed-histogram-information" tabindex="-1">Precomputed Histogram information <a class="header-anchor" href="#precomputed-histogram-information" aria-label="Permalink to &quot;Precomputed Histogram information&quot;">​</a></h4><p>Many preprocessing tasks in remote sensing and machine learning workflows require an understanding of histogram statistics. For example, normalizing pixel values between the 2.5th and 98.5th percentiles (p2.5 and p98.5) is a common practice to minimize the influence of outliers. Other typical applications include contrast stretching to enhance image visual quality and adaptive thresholding for feature detection based on intensity distribution.</p><p>Generating histograms across all samples in a dataset requires a forward pass through the entire data, which can be computationally expensive, especially for large-scale or nested datasets. While tools like GDAL provide commands such as <code>gdalinfo -hist -approx_stats -json</code> to extract histogram metadata directly from images, this approach still necessitates reading all metadata before accessing the actual statistics, limiting efficiency in streaming scenarios.</p><p>To overcome this limitation, we propose a new method for streaming datasets within TACO. Because TACO enforces that all samples share the same metadata schema and nested structure, we store precomputed histograms as TORTILLA with <code>special</code> TOG files. It is special because instead of the typical <code>tile=True</code> setting used for raster data, we save histograms with <code>tile=False</code>, storing them as stripped GeoTIFFs optimized for sequential reading. To simplify processing, histograms are standardized to a fixed width of 100 bins. The dataset values used to compute these histograms are normalized linearly between the global minimum and maximum of the dataset, then scaled to the 0–255 range. This quantization reduces precision but facilitates compact storage and efficient transmission. The primary goal is not to preserve absolute precision but to provide consistent, comparable histogram representations for normalization and statistical analysis. Utilizing this metadata, the TACO reader API can efficiently provide non-parametric statistics and histograms to users at the sample, subset, or entire dataset level.</p><p>TODO</p></div></div></main><footer class="VPDocFooter" data-v-39a288b8 data-v-e257564d><!--[--><!--]--><!----><!----></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!----><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"catalogue.md\":\"Bg9brqR1\",\"contribute.md\":\"C7eV4n2l\",\"documentation_community.md\":\"BzpuDTJA\",\"documentation_events.md\":\"D6Q5bAFK\",\"documentation_ideas.md\":\"SVrCt7cb\",\"documentation_index.md\":\"DHxI0FYx\",\"index.md\":\"BhGTxQUe\",\"publications.md\":\"CWAbRPpL\",\"specification.md\":\"gSG_sY5t\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"The TACO Specification\",\"description\":\"Every EO problem deserves a TACO.\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"nav\":[{\"text\":\"Home\",\"link\":\"/\"},{\"text\":\"Specification\",\"link\":\"/specification\"},{\"text\":\"Documentation\",\"link\":\"/documentation\"},{\"text\":\"Contributing\",\"link\":\"/contribute\"},{\"text\":\"Catalog\",\"link\":\"/catalogue\"},{\"text\":\"Publications\",\"link\":\"/publications\"}],\"sidebar\":{\"/documentation/\":[{\"text\":\"Documentation\",\"collapsed\":false,\"items\":[{\"text\":\"Community\",\"link\":\"/documentation/community\"},{\"text\":\"Events\",\"link\":\"/documentation/events\"},{\"text\":\"Ideas\",\"link\":\"/documentation/ideas\"}]}]},\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/tacofoundation\"},{\"icon\":\"hugging face\",\"link\":\"https://huggingface.co/tacofoundation\"}]},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>